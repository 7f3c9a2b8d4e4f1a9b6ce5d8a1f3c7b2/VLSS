### Title
Loss Tolerance Bypass via Stale Oracle Price Median with Recent Max Timestamp

### Summary
Switchboard aggregators initialized with unbounded `max_staleness_seconds` can compute stale median prices from predominantly old oracle updates while presenting a recent `max_timestamp_ms` that bypasses vault staleness checks. This allows vault operations to use incorrect asset valuations, enabling losses that exceed configured per-epoch tolerance limits without triggering safety aborts.

### Finding Description

The vulnerability exists in the interaction between three components:

**1. Unbounded Staleness Configuration:** [1](#0-0) 

The `validate()` function only checks `max_staleness_seconds > 0` without enforcing an upper bound. This allows aggregators to be initialized with extreme values like `u64::MAX` or years-long staleness windows (e.g., 31,536,000 seconds = 1 year).

**2. Median Computation from Mixed-Age Updates:** [2](#0-1) 

The `compute_current_result()` function collects all updates within the staleness window via `valid_update_indices()` and computes:
- `max_timestamp_ms` = maximum timestamp across ALL valid updates (line 392)
- `result` = median price across ALL valid updates (line 398)

When `max_staleness_seconds` is large, both very old and recent updates are included. The median is computed from this entire set, so if there are 10 old updates at $100 and 1 recent update at $200, the median returns $100 (stale). However, `max_timestamp_ms` returns the recent timestamp.

**3. Insufficient Staleness Check:** [3](#0-2) 

The `get_current_price()` function validates freshness by checking: `assert!(now - max_timestamp < config.update_interval, ERR_PRICE_NOT_UPDATED)` (line 259). This check passes because `max_timestamp_ms` is recent, even though the actual price being returned (the median) is computed from predominantly stale data.

**4. Loss Tolerance Enforcement Uses Stale Prices:** [4](#0-3) 

The vault operation lifecycle captures `total_usd_value_before` at operation start (line 353), then `total_usd_value_after` at operation end (line 355-357). The loss calculation (lines 361-363) depends on accurate USD valuations derived from oracle prices. [5](#0-4) 

The `get_total_usd_value()` function aggregates asset values, which are computed from oracle prices via functions like `update_free_principal_value()`. [6](#0-5) 

Asset value updates call `get_normalized_asset_price()` which ultimately uses the stale median price that passed the insufficient staleness check. [7](#0-6) 

The `update_tolerance()` function enforces: `assert!(loss_limit >= self.cur_epoch_loss, ERR_EXCEED_LOSS_LIMIT)` where `loss_limit = cur_epoch_loss_base_usd_value × loss_tolerance / RATE_SCALING`. When stale prices artificially inflate asset values, actual losses appear smaller than reality, bypassing this critical safety check.

### Impact Explanation

**Security Integrity Impact - Loss Tolerance Bypass:**
The loss tolerance mechanism is a critical safety feature limiting per-epoch losses to `loss_tolerance` (default 0.1% = 10 basis points). With stale prices:
- Actual market price drops from $100 to $50 (50% loss)
- Stale median price remains at $100 
- Vault calculates $0 loss instead of 50% loss
- Operations proceed despite exceeding configured tolerance limits

**Direct Fund Impact:**
Accumulated losses can exceed intended safety thresholds across multiple operations within an epoch. For a $10M vault with 0.1% tolerance:
- Intended max loss per epoch: $10,000
- With stale prices masking a 1% actual loss: $100,000 real loss
- 10x breach of safety limit, exposing users to unintended risk

**Affected Parties:**
- Vault depositors: funds exposed to losses beyond agreed risk parameters
- Protocol: reputation damage from safety mechanism failure
- Operators: may inadvertently violate risk limits without detection

### Likelihood Explanation

**Reachable Entry Point:**
The aggregator initialization is callable by anyone creating a Switchboard aggregator. While vault configuration is admin-controlled, if the protocol uses a third-party aggregator or misconfigures their own, this becomes exploitable.

**Feasible Preconditions:**
1. Aggregator configured with large `max_staleness_seconds` (intentionally or via misconfiguration)
2. Multiple oracle nodes provide updates over time
3. Market conditions cause significant price movements
4. Majority of oracles lag in updating to new prices while minority updates promptly

**Execution Practicality:**
- No special privileges required beyond normal vault operation
- Attack surfaces existing aggregator configurations (no need to create malicious aggregator if one exists)
- Natural market conditions (oracle update lags during volatility) can trigger this without intentional exploitation

**Economic Rationality:**
- For a $10M vault, bypassing 0.1% tolerance could enable $100K+ in additional losses
- Operators might unknowingly exploit this during high-volatility periods
- No direct cost to attacker if exploiting existing misconfigured aggregators

**Detection/Operational Constraints:**
The vulnerability is subtle because:
- Staleness checks appear to pass (recent max_timestamp)
- No obvious errors or alerts
- Losses only detected post-facto when actual price data is reviewed
- Time window for exploitation extends throughout epoch

### Recommendation

**Code-Level Mitigation:**

1. **Add Upper Bound to Staleness Validation:** [8](#0-7) 

```move
const MAX_ALLOWED_STALENESS_SECONDS: u64 = 300; // 5 minutes max

public fun validate(
    queue: &Queue,
    feed_hash: vector<u8>,
    min_sample_size: u64,
    max_staleness_seconds: u64,
    max_variance: u64,
    min_responses: u32,
) {
    assert!(queue.version() == EXPECTED_QUEUE_VERSION, EInvalidQueueVersion);
    assert!(min_sample_size > 0, EInvalidMinSampleSize);
    assert!(max_variance > 0, EInvalidMaxVariance);
    assert!(feed_hash.length() == 32, EInvalidFeedHash);
    assert!(min_responses > 0, EInvalidMinResponses);
    assert!(max_staleness_seconds > 0, EInvalidMaxStalenessSeconds);
    assert!(max_staleness_seconds <= MAX_ALLOWED_STALENESS_SECONDS, EInvalidMaxStalenessSeconds);
}
```

2. **Validate Median Timestamp in Vault Oracle:** [3](#0-2) 

Add check that the median update's timestamp (not just max_timestamp) is within acceptable bounds:

```move
public fun get_current_price(config: &OracleConfig, clock: &Clock, aggregator: &Aggregator): u256 {
    config.check_version();

    let now = clock.timestamp_ms();
    let current_result = aggregator.current_result();

    let max_timestamp = current_result.max_timestamp_ms();
    let median_timestamp = current_result.timestamp_ms(); // Timestamp of median value
    
    // Check both max and median timestamps
    if (now >= max_timestamp) {
        assert!(now - max_timestamp < config.update_interval, ERR_PRICE_NOT_UPDATED);
    };
    if (now >= median_timestamp) {
        assert!(now - median_timestamp < config.update_interval, ERR_PRICE_NOT_UPDATED);
    };
    
    current_result.result().value() as u256
}
```

3. **Invariant Checks:**
Add assertions that all individual updates used in median computation are within acceptable staleness bounds, not just checking boundary timestamps.

4. **Test Cases:**
Add regression tests for:
    - Aggregators with large `max_staleness_seconds` attempting to initialize
    - Scenarios with mixed old/new updates verifying median timestamp validation
    - Loss tolerance enforcement with artificially aged oracle data

### Proof of Concept

**Initial State:**
- Aggregator initialized with `max_staleness_seconds = 31,536,000` (1 year)
- Vault with $1M in assets, `loss_tolerance = 10` (0.1%)
- Asset priced at $100, 10 oracle nodes have submitted updates at this price

**Transaction Sequence:**

**T0 (Day 0):** 10 oracle updates submitted, all report price = $100
- Aggregator stores 10 updates at timestamp T0, all with result = $100

**T1 (Day 365):** Market price crashes to $50
- Only 1 oracle updates promptly with price = $50 at timestamp T1
- 9 old updates from T0 still valid (within 1-year staleness window)

**T2 (Day 365 + 1 minute):** Operator starts vault operation
1. Call `start_op_with_bag()` → captures `total_usd_value_before` 
2. During asset value update, calls `update_free_principal_value()`
3. This calls `get_normalized_asset_price()` → `get_asset_price()` → checks cached price
4. Price cache needs refresh, operator calls `update_price()`
5. `update_price()` → `get_current_price()` validates aggregator
6. Aggregator's `compute_current_result()`:
   - `valid_update_indices()` returns 10 indices (all within 1 year)
   - `max_timestamp_ms` = T1 (recent update)
   - `median_result()` = $100 (median of 9×$100 and 1×$50)
7. Staleness check: `now - T1 < 60,000ms` ✅ PASSES
8. Returns stale price: $100

**T3:** Operator completes operation
- Assets actually worth $500K (at real price $50)
- Vault calculates value as $1M (at stale price $100)
- Loss = $1M - $1M = $0 
- `update_tolerance(0)` → no loss recorded ✅ PASSES
- Expected: Should detect $500K loss (50%) and ABORT

**Expected Result:** Operation aborts with `ERR_EXCEED_LOSS_LIMIT` (50% loss >> 0.1% tolerance)

**Actual Result:** Operation succeeds with $0 recorded loss, bypassing loss tolerance protection

**Success Condition:** Vault operations complete despite actual losses exceeding configured tolerance limits by orders of magnitude.

### Citations

**File:** volo-vault/local_dependencies/switchboard_sui/on_demand/sources/actions/aggregator/aggregator_init_action.move (L29-43)
```text
public fun validate(
    queue: &Queue,
    feed_hash: vector<u8>,
    min_sample_size: u64,
    max_staleness_seconds: u64,
    max_variance: u64,
    min_responses: u32,
) {
    assert!(queue.version() == EXPECTED_QUEUE_VERSION, EInvalidQueueVersion);
    assert!(min_sample_size > 0, EInvalidMinSampleSize);
    assert!(max_variance > 0, EInvalidMaxVariance);
    assert!(feed_hash.length() == 32, EInvalidFeedHash);
    assert!(min_responses > 0, EInvalidMinResponses);
    assert!(max_staleness_seconds > 0, EInvalidMaxStalenessSeconds);
}
```

**File:** volo-vault/local_dependencies/switchboard_sui/on_demand/sources/schemas/aggregator.move (L338-412)
```text
fun compute_current_result(aggregator: &Aggregator, now_ms: u64): Option<CurrentResult> {
    let update_state = &aggregator.update_state;
    let updates = &update_state.results;
    let mut update_indices = update_state.valid_update_indices(aggregator.max_staleness_seconds * 1000, now_ms);

    // if there are not enough valid updates, return
    if (update_indices.length() < aggregator.min_sample_size) {
        return option::none()
    };

    // if there's only 1 index, return the result
    if (update_indices.length() == 1) {
        let (result, timestamp_ms) = update_state.median_result(&mut update_indices);
        return option::some(CurrentResult {
            min_timestamp_ms: updates[update_indices[0]].timestamp_ms,
            max_timestamp_ms: updates[update_indices[0]].timestamp_ms,
            min_result: result,
            max_result: result,
            range: decimal::zero(),
            result,
            stdev: decimal::zero(),
            mean: result,
            timestamp_ms,
        })
    };

    let mut sum: u128 = 0;
    let mut min_result = decimal::max_value();
    let mut max_result = decimal::zero();
    let mut min_timestamp_ms = u64::max_value!();
    let mut max_timestamp_ms = 0;
    let mut mean: u128 = 0;
    let mut mean_neg: bool = false;
    let mut m2: u256 = 0;
    let mut m2_neg: bool = false;
    let mut count: u128 = 0;

    vector::do_ref!(&update_indices, |idx| {
        let update = &updates[*idx];
        let value = update.result.value();
        let value_neg = update.result.neg();
        count = count + 1;

        // Welford's online algorithm
        let (delta, delta_neg) = sub_i128(value, value_neg, mean, mean_neg);
        (mean, mean_neg) = add_i128(mean, mean_neg, delta / count, delta_neg);
        let (delta2, delta2_neg) = sub_i128(value, value_neg, mean, mean_neg);

        (m2, m2_neg) = add_i256(m2, m2_neg, (delta as u256) * (delta2 as u256), delta_neg != delta2_neg);

        sum = sum + value;
        min_result = decimal::min(&min_result, &update.result);
        max_result = decimal::max(&max_result, &update.result);
        min_timestamp_ms = u64::min(min_timestamp_ms, update.timestamp_ms);
        max_timestamp_ms = u64::max(max_timestamp_ms, update.timestamp_ms);
    });

    let variance = m2 / ((count - 1) as u256); 
    let stdev = sqrt(variance);
    let range = max_result.sub(&min_result);
    let (result, timestamp_ms) = update_state.median_result(&mut update_indices);
    
    // update the current result
    option::some(CurrentResult {
        min_timestamp_ms,
        max_timestamp_ms,
        min_result,
        max_result,
        range,
        result,
        stdev: decimal::new(stdev, false),
        mean: decimal::new(mean, false),
        timestamp_ms,
    })
}
```

**File:** volo-vault/sources/oracle.move (L250-262)
```text
public fun get_current_price(config: &OracleConfig, clock: &Clock, aggregator: &Aggregator): u256 {
    config.check_version();

    let now = clock.timestamp_ms();
    let current_result = aggregator.current_result();

    let max_timestamp = current_result.max_timestamp_ms();

    if (now >= max_timestamp) {
        assert!(now - max_timestamp < config.update_interval, ERR_PRICE_NOT_UPDATED);
    };
    current_result.result().value() as u256
}
```

**File:** volo-vault/sources/operation.move (L353-377)
```text
    let total_usd_value_before = total_usd_value;
    vault.check_op_value_update_record();
    let total_usd_value_after = vault.get_total_usd_value(
        clock,
    );

    // Update tolerance if there is a loss (there is a max loss limit each epoch)
    let mut loss = 0;
    if (total_usd_value_after < total_usd_value_before) {
        loss = total_usd_value_before - total_usd_value_after;
        vault.update_tolerance(loss);
    };

    assert!(vault.total_shares() == total_shares, ERR_VERIFY_SHARE);

    emit(OperationValueUpdateChecked {
        vault_id: vault.vault_id(),
        total_usd_value_before,
        total_usd_value_after,
        loss,
    });

    vault.set_status(VAULT_NORMAL_STATUS);
    vault.clear_op_value_update_record();
}
```

**File:** volo-vault/sources/volo_vault.move (L626-641)
```text
public(package) fun update_tolerance<T0>(self: &mut Vault<T0>, loss: u256) {
    self.check_version();

    self.cur_epoch_loss = self.cur_epoch_loss + loss;

    // let loss_limit = usd_value_before * (self.loss_tolerance as u256) / (RATE_SCALING as u256);
    let loss_limit =
        self.cur_epoch_loss_base_usd_value * (self.loss_tolerance as u256) / (RATE_SCALING as u256);

    assert!(loss_limit >= self.cur_epoch_loss, ERR_EXCEED_LOSS_LIMIT);
    emit(LossToleranceUpdated {
        vault_id: self.vault_id(),
        current_loss: self.cur_epoch_loss,
        loss_limit: loss_limit,
    });
}
```

**File:** volo-vault/sources/volo_vault.move (L1101-1128)
```text
public fun update_free_principal_value<PrincipalCoinType>(
    self: &mut Vault<PrincipalCoinType>,
    config: &OracleConfig,
    clock: &Clock,
) {
    self.check_version();
    self.assert_enabled();

    let principal_price = vault_oracle::get_normalized_asset_price(
        config,
        clock,
        type_name::get<PrincipalCoinType>().into_string(),
    );

    let principal_usd_value = vault_utils::mul_with_oracle_price(
        self.free_principal.value() as u256,
        principal_price,
    );

    let principal_asset_type = type_name::get<PrincipalCoinType>().into_string();

    finish_update_asset_value(
        self,
        principal_asset_type,
        principal_usd_value,
        clock.timestamp_ms(),
    );
}
```

**File:** volo-vault/sources/volo_vault.move (L1254-1279)
```text
public(package) fun get_total_usd_value<PrincipalCoinType>(
    self: &Vault<PrincipalCoinType>,
    clock: &Clock,
): u256 {
    self.check_version();
    self.assert_enabled();

    let now = clock.timestamp_ms();
    let mut total_usd_value = 0;

    self.asset_types.do_ref!(|asset_type| {
        let last_update_time = *self.assets_value_updated.borrow(*asset_type);
        assert!(now - last_update_time <= MAX_UPDATE_INTERVAL, ERR_USD_VALUE_NOT_UPDATED);

        let usd_value = *self.assets_value.borrow(*asset_type);
        total_usd_value = total_usd_value + usd_value;
    });

    emit(TotalUSDValueUpdated {
        vault_id: self.vault_id(),
        total_usd_value: total_usd_value,
        timestamp: now,
    });

    total_usd_value
}
```
